---
title: Text Detection思考
layout: post
date: 2019-08-19
categories: 
- paper_reading
tag: paper
blog: true
start: true
author: karl
description: Scene Text Detection





---

上半年的文字检测算法还是集中于one-stage和two-stage。one-stage的算法主要利用FPN的结构预测分割图以及其他辅助的标签，如回归的值等得到文本的区域；two-stage的算法主要利用mask-rcnn辅助一些改进提升文字检测算法的性能。这篇文章我们就主要说一说这半年的文字检测算法到底在改进什么。

### one-stage算法

这类算法多为基于unet分割的优势，讲文本检测任务转换为文本区域分割的问题，同时利用分割时像素的位置信息，预测一些相关定位的信息，最后将信息整合，得到最终的文本区域。

上半年里one-stage的文字检测算法代表，CRAFT和PSENet。两者网络结构如下：



|                            CRAFT                             |                  PSENet                  |
| :----------------------------------------------------------: | :--------------------------------------: |
| ![img](https://media.arxiv-vanity.com/render-output/1112933/x3.png) | ![img](/Users/gengjiajia/Desktop/x2.png) |

两种算法可以认为是两条路线：

1. 在分割的基础上，像素不仅仅预测类别，同时预测与位置相关的信息。
2. 利用像素的分割信息寻找文本实例。

这里分别对这两种类型进行分析。首先是第一种类型的检测算法。这类型类似anchor free的算法，在模型的最后一层输出像素的分类信息以及回归信息。EAST、AdvancedEAST以及一些类EAST的算法，都是如此的做法。但是EAST等直接预测一个文本实例的方法容易受特征的局限，进而导致最终长距离无法稳定预测。于是后来目光逐渐转向slices+grouping的思路。pixellink，textfield， textmoutain等。这类型算法因为只需要关注局部相关的信息就可以很好的得到文本实例，缺点是如果局部信息获取不好，检测会出现badcases。但是这类算法在榜上都刷到了不错的性能，但是一旦把回归方式改为预测整个文本实例的位置信息，性能就会下降。为什么？

原因在于仅仅凭借unet的最后一层输出，去预测文本的各种尺度大小，是没有这样的特征的。如stride=4的feature，经过conv3x3或许可以cover住12x12的范围，但是这时候用这些特征如预测512x128的文本实例，是会出现问题的。因此，大家换做局部slices+grouping的做法，尽可能的让单层特征预测尺度变化不大的目标，然后利用grouping得到最终的文本实例。

结合分割与像素回归的方法，边界会相对稳定，因为其边界多为很多像素投票或者加权的结果。也因为如此，直接回归文本实例的方法，无法很好的精准定位长文本的边界。



对于第二种算法，则需要借助网络超强的分割能力，尽可能精准的将文本区域分割出来，接着利用像素之间的一些关系，得到文本实例。这类型算法依赖很强的特征提取能力，需要提取到可以很好的判别文本像素归属的特征，因此依赖unet的结构，如结合多尺度信息融合的手段增强特征，分割文本。但是由于文本区域中是包含背景噪声的，因此当文本包含背景噪声较多时，如镂空、较大的字体、较大的间隔等，容易导致检测的失败。



以上是对one-stage方法的简单分析，one-stage算法需要面对的是类别不均衡、特征misalignment等问题，当然与文本标签中存在部分噪声有关系。



### Two-stage算法

two-stage算法的典型代表便是基于maskrcnn的改进，这里拿如下两种算法进行分析：

|                  Box_Discretization_Network                  |                   PMTD                   |
| :----------------------------------------------------------: | :--------------------------------------: |
| ![img](https://media.arxiv-vanity.com/render-output/1089570/x1.png) | ![img](/Users/gengjiajia/Desktop/x3.png) |

我们知道MaskRCNN的回归方式为，roi_feature进行编码，预测类别，bbox以及分割图。但是由于文本的宽高比变化剧烈，长文本会无法cover。因此以上两种算法对maskrcnn的结果进行微调。即去预测辅助定位的信息。BDN将ROIAlign得到的特征分别预测三种信息，分别为水平方向的响应值，竖直方向的响应值，以及xy的匹配方式。这里具体的方式可以阅读论文参看细节。为什么这么做管用？这里的分类的目标值为bbox定点与中心点的平均，如(xmin + xmean) / 2, 将四个顶点与中心点关联，同时这个预测值往往会落在roi范围内，避免了直接回归很远的位置，利用现有范围内的特征预测得到范围外的值。最终在得出检测结果是，进行qbox的校准。

对于PMTD，则是将mask分支的分割标签改进，改为类似于高斯图的结构，这样做的目的是为了在检测得到文本区域之后，可以利用分割得到的值进行边界的推断，最终得到完整的文本区域。



### 一些想法

实际上Maskrcnn做文字检测可以达到一个很高的baseline，如PMTD中说的maskrcnn在icdar2017mlt上可以到76%以上，而回过头来看one-stage算法在mlt上的性能，psenet性能为72%左右，相当于one-stage的极限。是什么原因导致了这样的结果？如果把backbone对齐的话，我们可以看到maskrcnn相比较psenet等算法多做了RPN，ROIAlign，Instance-Segmentation，这些部件的作用可以如下类比，RPN相当于one-stage中的直接回归，ROIAlign特征对齐，Instance-Segmentation是类别均衡的分割。对比之下，可以看出以下几点：

- 特征对齐对于直接回归的重要性，假设EAST中每个像素与整个文本实例相关，那么可以很好的定位边界。
- 特征感受野的特征回归特定的范围，如果类EAST算法更改回归方式，可以很好的避免回归目标与特征不匹配的问题。

以上只是对比one-stage与two-stage，而对于maskrcnn来说，依旧存在对于宽高比剧烈的文本无法检测完整，原因在于此类文本经过ROIAlign得到的特征，往往只是局部信息，因此无法很好的回归整个文本实例。那么maskrcnn应该如何改进呢？我觉得还是特征对齐～

